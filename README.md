# Hi there, I'm Vaishnavi Singh üëã

### üá∏üá¨ Context
I am a final-year student at **Singapore Management University (SMU)**, studying the intersection of **Computer Science, Law, and Political Science**.

### üî≠ Mission
My goal for the next decade is to bridge the gap between **Technical AI Safety** and **AI Governance**. I believe effective regulation requires a deep technical understanding of model failure modes, and effective technical safety requires robust international coordination frameworks.

### üß† Research Focus Areas

#### 1. Technical Alignment: Evaluations & Scheming
I am trying to learn more about **Model Evals**. I am curious about benchmarks that go beyond measuring simple "goal failure" or task performance, focusing instead on cases detecting evidence of **Instrumental Convergence** and latent capabilities:

* **Power-Seeking:** Testing for behaviors where models attempt to acquire resources (compute, financial access) or prevent their own shutdown, regardless of the assigned task's outcome. Shutdown avoidance is a rather curious and terrifying concept to me.
* **Scheming & Deception:** Reading about deceptively aligned models that sandbag, covertly strategise and display unique strategic thinking.
* **Situational Awareness:** Evaluating models for "out-of-context" reasoning‚Äîdetecting when they are in training vs. deployment. Some overlaps here with my interest in personhood and the idea of "self-awareness". 

#### 2. AI Control & Robustness
Heavily inspired by the work of **Ryan Greenblatt (Redwood Research)**, I am interested in **AI Control** techniques. specifically, methods to safely monitor and elicit work from untrusted models without relying on them being fully aligned.
* **Key Interest:** Protocol design for monitoring untrusted advanced systems (e.g., debate, cross-examination protocols). Also, the role of scalable oversight. 

#### 3. AI Governance & Strategy
Leveraging my legal and political science background, I am exploring:
* **International Red-Lines:** How to push US and China towards shared safety agreements to prevent mutually assured destruction.
* **Compute Governance:** Navigating data center security and sustainable energy usage, specifically from the perspective of **Singapore's unique neutral and non-aligned status**.

#### 4. Philosophy of Mind
* **AI Personhood:** Investigating the criteria for acknowledging silicon-based "beings" and the legal/ethical frameworks required to support such a shift. Could we need to figure out a theory of co-alignment? What would this entail

### üìç Achievemnts + Current Trajectory 
* I have finished multiple fellowships and courses.
* I am a Berkeley AI Safety Fellow, my project with Desiree Junfijiah and Stephanie Choi is on a practical theory of AI Moral Personhood.
  * Our short paper has been accepted to FAST Workshop @ AAAI 2026 in Singapore, my home. We aim to publish a long paper early-mid 2026.
* I have run the NUS AI Alignment Initiatives' 2 Fellowships (AI Governance and Technical Tracks) as a Leading Co-organiser under the Pathfinder Fellowship. 
Most recently, I was a participant in the **BlueDot Impact AGI Safety Fundamentals (Strategy)** cohort.

My immediate roadmap involves rigorous technical self-study to bridge the gap between strategy and engineering. I am actively self-learning/applying to ML bootcamps to improve my technical skills. My goal in 1-2 years time is to **apply for grants/RA positions** to conduct AI Control Evals + Experiments for testing control protocols on current frontier models based on future work outlined by METR, Epoch, LASRLabs, Redwood.

### üõ†Ô∏è Technical Stack
* **Languages:** Python, SQL
* **Safety Tools:** PyTorch, TransformerLens
* **Governance:** Policy Analysis, Policy Design, International Law Frameworks,

### An unexamined life is not one worth living
* I am an aspiring Acid/Industrial Techno DJ. SoundCloud link incoming? 
* I bartend at a speakeasy where I break a lot of glasses (sadly).
* I have a podcast focusing on religious philophy.
* My goal in life is to learn as much as I can, and to one day be detached from material pursuits. 

---
*Connect with me to discuss how we can secure a safe AI future.*



