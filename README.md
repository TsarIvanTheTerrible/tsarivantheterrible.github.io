# Hi there, I'm Vaishnavi Singh üëã

<small>
I am a final-year student at <b>Singapore Management University (SMU)</b>, studying the intersection of <b>Computer Science, Law, and Political Science</b>.

My goal for the next decade is to bridge the gap between <b>Technical AI Safety</b> and <b>AI Governance</b>. I believe effective regulation requires a deep technical understanding of model failure modes, and effective technical safety requires robust international coordination frameworks.
</small>

---

### üß† Research Focus Areas

<small>
<b>1. Technical Alignment: Evaluations & Scheming</b><br>
I focus on benchmarks that go beyond simple task performance, detecting evidence of <b>Instrumental Convergence</b> and latent capabilities.
</small>
<br>
<details>
<summary><b> Evals & Awareness</b></summary>
<br>
<small>
<ul>
<li><b>Power-Seeking & Shutdown Avoidance:</b> Testing for behaviors where models attempt to acquire resources or prevent their own shutdown. <i>(Shutdown avoidance is a particularly curious and terrifying concept to me).</i></li>
<li><b>Scheming & Deception:</b> Studying deceptively aligned models that sandbag, covertly strategize, and display unique strategic thinking.</li>
<li><b>Situational Awareness:</b> Evaluating models for "out-of-context" reasoning. This overlaps with my interest in personhood and the idea of "self-awareness."</li>
</ul>
</small>
</details>

<br>

<small>
<b>2. AI Control & Robustness</b><br>
Heavily inspired by the work of <b>Ryan Greenblatt (Redwood Research)</b>, I am interested in methods to safely monitor and elicit work from untrusted models.
</small>
<br>
<details>
<summary><b> Control Protocols</b></summary>
<br>
<small>
<ul>
<li><b>Protocol Design:</b> I want to looka t debate and cross-examination protocols for untrusted advanced systems.</li>
<li><b>Scalable Oversight:</b> I want to investigate how we can reliably supervise models that are smarter than us.</li>
</ul>
</small>
</details>

<br>

<small>
<b>3. AI Governance & Strategy</b><br>
Leveraging my legal and political science background, I focus on international coordination and compute governance.
</small>
<br>
<details>
<summary><b> Policy & Singapore Context</b></summary>
<br>
<small>
<ul>
<li><b>International Red-Lines:</b> Pushing US and China towards shared safety agreements to prevent mutually assured destruction.</li>
<li><b>Compute Governance:</b> Navigating data center security and sustainable energy usage, specifically from the perspective of <b>Singapore's unique neutral status</b>.</li>
</ul>
</small>
</details>

<br>

<small>
<b>4. Philosophy of Mind</b><br>
Investigating the criteria for acknowledging silicon-based "beings" and the legal frameworks required.
</small>
<br>
<details>
<summary><b> Personhood & Co-alignment</b></summary>
<br>
<small>
<ul>
<li><b>AI Personhood:</b> Continuing short paper's work on investigating the criteria for acknowledging silicon-based "beings."</li>
<li><b>Co-alignment:</b> Exploring if we need a theory of "co-alignment" rather than just alignment, and what that would entail technically, grounded in ethical theories </li>
</ul>
</small>
</details>

---

### üèÜ Achievements & Fellowships

<small> <ul>
<li><b>Berkeley AI Safety Fellow:</b> Developing a practical theory of AI Moral Personhood leveraging current evals with Desiree Junfijiah and Stephanie Choi.
<br>‚Ü≥ <i>Short paper accepted to <b>FAST Workshop @ AAAI 2026</b> (Singapore). Targeting a long paper for mid-2026.</i></li>
<li><b>Pathfinder Fellow (NUS AI Alignment Initiatives):</b> Leading Co-organiser running 2 Fellowships (AI Governance and Technical Tracks).</li>
<li><b>BlueDot Impact:</b> Alumna of the AGI Safety Fundamentals (Strategy) cohort.</li>
</ul> </small>

---

### üìç Roadmap & Current Trajectory

<small>
<b>Current Semester Goals:</b>
<ul>
<li>üöÄ <b>Final Year Project:</b> Collaborating with <b>Confide Platform</b> to create live AI Governance tooling for large corporations.</li>
<li>üìù <b>Publication:</b> Finalizing the long paper on how evals can help us figure out personhood.</li>
<li>üéì <b>PhD Strategy:</b> actively scoping PhD opportunities in Safety/Governance.</li>
</ul>
</small>

<small> 
<b>1-2 Year Horizon:</b>
<ul>
<li><b>Research & Grants:</b> Applying for grants/RA positions to conduct <b>AI Control Evals</b> on frontier models (referencing work by METR, Epoch, LASR Labs, Redwood).</li>
<li><b>Technical Rigor:</b> Conducting rigorous self-study and applying to ML bootcamps to bridge gaps in my knowledge (you can only walk the path alone so far!).</li>
</ul>
</small>

---

### üí≠ "The Unexamined Life..."

<small>
<i>I believe in living a life of curiosity and eventual detachment from material pursuits.</i>
<ul>
<li>üéß <b>DJing:</b> Aspiring Acid/Industrial Techno DJ. (SoundCloud link incoming).</li>
<li>üç∏ <b>Bartending:</b> I bartend at a speakeasy (and arguably break too many glasses).</li>
<li>üéôÔ∏è <b>Podcasting:</b> I host a podcast focusing on religious philosophy.</li>
</ul>
</small>

---
<div align="center">
<small><i>Connect with me to discuss how we can secure a safe AI future.</i></small>
</div>


