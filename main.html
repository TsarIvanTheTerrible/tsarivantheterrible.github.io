
# Hi there, I'm Vaishnavi Singh üëã

<small>I am a final-year student at Singapore Management University (SMU), studying the intersection of Computer Science, Law, and Political Science.
  
  I hope to work on both Technical AI Safety and AI Governance. I believe effective regulation requires a deep technical understanding of model failure modes, and effective technical safety requires working with international coordination frameworks.
</small>

---

### üß† Research Interests

<small> I am building my technical toolkit to understand benchmarks that go beyond simple task performance, specifically looking for evidence of instrumental convergence and latent capabilities. </small>
<br>

<details>
<summary> Evals & Awareness</summary>
<br>
<small>
<ul>
  <li>I am studying how to test for power-seeking behaviors where models attempt to acquire resources or prevent their own shutdown. The concept of shutdown avoidance is particularly compelling to me.</li>
  <li>I am reading about deceptively aligned models to understand how they might sandbag, covertly strategize, or display unique strategic thinking.</li>
  <li>I am hoping to learn how to evaluating models for "out-of-context" reasoning to detect situational awareness, which overlaps with my interest in the philosophical definitions of self-awareness.</li>
</ul>
</small>
</details>

<br>

<small>
Heavily inspired by the work of researchers like Ryan Greenblatt and his posts on LW, I am interested in learning methods to safely monitor and elicit work from untrusted models.
</small>
<br>
<details>
<summary> Control Protocols</summary>
<br>
<small>
<ul>
<li>I aim to research protocol designs such as debate and cross-examination suited for untrusted advanced systems.</li>
<li>I want to investigate how we can reliably supervise models that are smarter than us through scalable oversight mechanisms.</li>
</ul>
</small>
</details>

<br>

<small>
Leveraging my legal and political science background, I am exploring international coordination and compute governance.
</small>
<br>
<details>
<summary>Policy + Singapore Context</summary>
<br>
<small>
<ul>
<li>I am analyzing mechanisms to push the US and China towards shared safety agreements to prevent mutually assured destruction.</li>
<li>I am researching data center security and sustainable energy usage, specifically analyzing how this fits into Singapore's unique neutral status.</li>
</ul>
</small>
</details>

<br>

<small>
I am investigating the criteria for acknowledging silicon-based "beings" and the legal frameworks that would be required.
</small>
<br>
<details>
<summary>Practical = Palatable AI Personhood & Co-alignment </summary>
<br>
<small>
<ul>
  <li>I am continuing work from a short paper to investigate the criteria for acknowledging silicon-based "beings."</li>
  <li>I am exploring if we need a theory of "co-alignment" rather than just alignment, and what that would entail technically when grounded in ethical theories.</li>
</ul>
</small>
</details>

---

### üèÜ Fellowships & Early Work

<small> 
  <ul> 
    <li>As a Berkeley AI Safety Fellow, I am developing a practical theory of AI Moral Personhood leveraging current evals with Desiree Junfijiah and Stephanie Choi. Our short paper was accepted to the FAST Workshop at AAAI 2026 in Singapore, and we are targeting a long paper for mid-2026.</li>
    <li>I served as a Pathfinder Fellow for the NUS AI Alignment Initiatives, where I was a leading co-organiser running the AI Governance and Technical fellowship tracks </li>
    <li>I am an alumna of the BlueDot Impact AGI Safety Fundamentals (Strategy) cohort.</li> 
  </ul> 
</small>

---

### üìç Roadmap

<small>
I am currently focused on the following goals for this semester:
  <ul>
    <li>I am collaborating with Confide Platform on my final year project to create live AI Governance tooling for large corporations.</li>
    <li>I am finalizing my long paper on how evals can help us figure out personhood.</li>
    <li>I am actively scoping PhD opportunities in Safety and Governance </li>
  </ul>
</small>

<small>¬†
Looking at a 1-2 year horizon:
  <ul>
    <li>I plan to apply for grants and research assistant positions to conduct AI Control evals on frontier models, referencing work by labs such as METR, Epoch, LASR Labs, and Redwood.</li>
    <li>I am conducting rigorous self-study and applying to ML bootcamps to bridge gaps in my technical knowledge, as I transition into full-time research.</li>
  </ul>
</small>

---

### üí≠ "The Unexamined Life" is a huge fear of mine

<small> I believe in living a life of curiosity and eventual detachment from material pursuits.
  <ul>
    <li>I am an aspiring Acid/Industrial Techno DJ (SoundCloud link incoming).</li>
    <li>I bartend at a speakeasy and arguably break too many glasses.</li>
    <li>I host a podcast focusing on religious philosophy.</li>
  </ul>
</small>

---
<div align="center">
<small><i>Connect with me to discuss how we can secure a safe AI future.</i></small>
</div>



